{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZPO6.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wojdzi1607/ColabNotebooks/blob/main/ZPO6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymPKP9oJoioj"
      },
      "source": [
        "# Zaawansowane przetwarzanie obrazu\n",
        "\n",
        "## Ćwiczenie laboratoryjne 6 – sieci neuronowe w PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lk7zZBgo23f"
      },
      "source": [
        "## Klasyfikacja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwf9QFVmo5dN"
      },
      "source": [
        "Utwórzmy prosty, sekwencyjny model sieci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pOSdXpFpLch"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=4096, out_features=64),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(in_features=64, out_features=32),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(in_features=32, out_features=10)\n",
        "    # Uwaga - nie używamy własnej funkcji aktywacji na koniec - wybierzemy loss function z wbudowaną\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrDLL2ZqrCpd"
      },
      "source": [
        "Podobnie jak w TensorFlow/Keras wybierzmy zbiór danych CIFAR10 oraz podstawowe augmentacje:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNzsJKX1rBsv"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "augmentations = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomAffine(degrees=10, translate=(0, 0.1), scale=(0.95, 1.05)),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='CIFAR10', download=True, transform=augmentations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQzVsA0Il3bV"
      },
      "source": [
        "print(dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ZUSD3ftELb"
      },
      "source": [
        "Podzielmy zbiór na treningowy i walidacyjny:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB2V91P9tDBe"
      },
      "source": [
        "valid_length = int(0.15 * len(dataset))\n",
        "train_length = len(dataset) - valid_length\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, (train_length, valid_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsBS7xI4uBkh"
      },
      "source": [
        "Utwórzmy funkcję kosztu oraz optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHbdcK_QuAw7"
      },
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83Z4yjWuhQf"
      },
      "source": [
        "Utwórzmy loader danych:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXRn-vv8ujxh"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=256, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs0hP-PzqMhE"
      },
      "source": [
        "next(iter(train_loader))[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stHwft2rugTv"
      },
      "source": [
        "Rozpocznijmy trening:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI6ZcDHJu60h"
      },
      "source": [
        "for epoch in range(30):\n",
        "  for i, (data_batch, labels_batch) in enumerate(train_loader):\n",
        "    predictions = model(data_batch)\n",
        "    loss = loss_function(predictions, labels_batch)\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'\\rStep {i + 1}/{len(train_loader)}: loss={loss.item():.4f}', end='')\n",
        "  print()\n",
        "\n",
        "  # with torch.no_grad():\n",
        "  #   losses = []\n",
        "  #   for i, (data_batch, labels_batch) in enumerate(train_loader):\n",
        "  #     data_batch = data_batch.to(device)\n",
        "  #     labels_batch = labels_batch.to(device)\n",
        "      \n",
        "  #     predictions = model(data_batch)\n",
        "  #     loss = loss_function(predictions, labels_batch)\n",
        "  #     losses.append(loss.item())\n",
        "    \n",
        "  #   print(f'Epoch {epoch + 1}: val_loss={torch.mean(torch.tensor(losses))}') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLdX9LqEu5zi"
      },
      "source": [
        "Spróbujmy przeprowadzić ten sam trening z wykorzystaniem GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0fYYyGtxK2p"
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "\n",
        "model = model.to(device)\n",
        "for epoch in range(30):\n",
        "  losses = []\n",
        "  for i, (data_batch, labels_batch) in enumerate(train_loader):\n",
        "    data_batch = data_batch.to(device)\n",
        "    labels_batch = labels_batch.to(device)\n",
        "    \n",
        "    predictions = model(data_batch)\n",
        "    loss = loss_function(predictions, labels_batch)\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    print(f'\\rStep {i + 1}/{len(train_loader)}: loss={torch.mean(torch.tensor(losses)):.4f}', end='')\n",
        "  print()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    losses = []\n",
        "    for i, (data_batch, labels_batch) in enumerate(valid_loader):\n",
        "      data_batch = data_batch.to(device)\n",
        "      labels_batch = labels_batch.to(device)\n",
        "      \n",
        "      predictions = model(data_batch)\n",
        "      loss = loss_function(predictions, labels_batch)\n",
        "      losses.append(loss.item())\n",
        "    \n",
        "    print(f'Epoch {epoch + 1}: val_loss={torch.mean(torch.tensor(losses)):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF6otAET1XCr"
      },
      "source": [
        "## Segmentacja, własne wczytywanie danych, PyTorch Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG69oymi2HXa"
      },
      "source": [
        "Pobierzmy i rozpakujmy zbiór danych:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPgMxaZU2CUr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip -q \"/content/drive/MyDrive/Colab_Files/oxford_pets_dogs_only_binarized_split.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3T9qedv2tad"
      },
      "source": [
        "Napiszmy własną klasę `Dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKdkPp9d2zKh"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, paths: List[Path], labels_dir: List[Path], augment: bool = False):\n",
        "      self._paths = paths\n",
        "      self._labels_dir = labels_dir\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "      return len(self._paths)\n",
        "\n",
        "  def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    path = self._paths[index]\n",
        "    mask_path = self._labels_dir / path.name\n",
        "    \n",
        "    input_image = np.asarray(Image.open(str(path)).resize((256, 256))) / 255\n",
        "    input_mask = np.asarray(Image.open(str(mask_path)).resize((256, 256), resample=Image.NEAREST))\n",
        "\n",
        "    return torch.from_numpy(np.moveaxis(input_image.astype(np.float32), -1, 0)), torch.from_numpy(input_mask.squeeze().astype(np.float32))[None, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRRsJWrF4_ao"
      },
      "source": [
        "Utwórzmy model PyTorch Lightning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQw1P8fO5NqV"
      },
      "source": [
        "!pip install -U pytorch-lightning segmentation-models-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceresQBz5Eqr"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from segmentation_models_pytorch import Unet\n",
        "from argparse import Namespace\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class Segmenter(pl.LightningModule):\n",
        "    def __init__(self, hparams: Namespace):\n",
        "        super().__init__()\n",
        "\n",
        "        self.network = Unet(in_channels=3, encoder_name='efficientnet-b0')\n",
        "\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "        paths = [path for path in Path('oxford_pets_dogs_only_binarized_split/train/images').iterdir()]\n",
        "        train_paths, valid_paths = train_test_split(paths, test_size=0.15)\n",
        "\n",
        "        self.train_dataset = SegmentationDataset(train_paths, Path('oxford_pets_dogs_only_binarized_split/train/labels'), augment=True)\n",
        "        self.valid_dataset = SegmentationDataset(valid_paths, Path('oxford_pets_dogs_only_binarized_split/train/labels'))\n",
        "\n",
        "        self.hparams = hparams\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.forward(x)\n",
        "        loss = self.loss(y_pred, y)\n",
        "        self.log('train_loss', loss, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.forward(x)\n",
        "        self.log('val_loss', self.loss(y_pred, y), on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "        reduce_lr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
        "                                                                          patience=3, min_lr=1e-6, verbose=True)\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': reduce_lr_on_plateau,\n",
        "            'monitor': 'train_loss'\n",
        "        }\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset, batch_size=64, num_workers=2, pin_memory=True, drop_last=True, shuffle=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.valid_dataset, batch_size=128, num_workers=2, pin_memory=True\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j81yTAbv7HuP"
      },
      "source": [
        "Rozpocznijmy trening:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zcY_qio7KGE"
      },
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "hparams = Namespace(lr=1e-3)\n",
        "model = Segmenter(hparams)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(filepath='{epoch}-{val_loss:.5f}', verbose=True)\n",
        "\n",
        "trainer = pl.Trainer(callbacks=[checkpoint_callback],\n",
        "                     gpus=-1, progress_bar_refresh_rate=1, max_epochs=50)\n",
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}